# -*- coding: utf-8 -*-
"""Proyek Pertama.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R6F0sFKoIJvGQJx4DXm5MZ1W2AizA8ot
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
from pandas.plotting import scatter_matrix

# %matplotlib inline

"""# Memahami Data

Memahami data ini bertujuan untuk mendapatkan insight-insight terhadap data.
"""

df = pd.read_csv("Metro_Interstate_Traffic_Volume.csv")

"""## Melihat Sekilas Struktur Data Dataset

Pada bagian ini dapat dilihat bahwa dataset terdiri dari 9 atribut, meliputi:
* holiday
* temp
* rain_1h
* snow_1h
* weatehr_main
* weather_description
* date_time
* traffic_volume
"""

df.head(10)

"""## Melihat tipe data value yang ada pada setiap atribut

dengan menggunakan metode .info(), kita dapat menyimpulkan beberapa hal:
1. Terdapat tipe data objek, hal ini umumnya menunjukan bahwa berupa kategorikal, perlu dianalisis lebih lanjut
2. Semua attribut memiliki 48204 instansi, hal ini menandakan tidak terdapatnya missing value
"""

df.info()

"""## Memperhatikan Nilai Statistik Dataset

Hal ini dilakukan untuk memeriksa nilai value pada dataset (apakah ada outlier/anomali) jika sudah memahami atribut yang ada pada dataset.
Sejauh ini dapat diambil kesimpulan bahwa ukuran yang digunakan pada suhu (temp) adalah kelvin.
"""

df.describe()

"""# Data Cleaning

## Ditemukan Data Tidak Normal

Sesuai dugaan tadi, terdapat nilai value temp 0 Kelvin, hal ini dapat mengganggu prediksi, maka dari itu data ini dapat dihapuskan nanti. Data ini dapat dihapus karena hilangnya data tersebut tidak mempengaruhi dataset secara keseluruhan.
"""

df[df["temp"] < 243]

## drop temperature yang hanya bernilai 0 kelvin
df.drop(df[df["temp"] < 243].index, inplace=True)
df.describe()

"""## Memeriksa Tipe Data Selain Numerik

Pada Cell sebelumnya terdapat data yang bertipekan objek yaitu, holiday, weather_main, dan weather_description
* Pada fitur holiday, kategori didominasi pada None/Bukan hari libur
* Pada fitur weather_main, merupakan kategori cuaca dengan deskripsi singkat
* Pada fitur weater_description, memiliki banyak kategori dengan deskripsi lebih lanjut daripada weather_main
"""

# df["holiday"].value_counts()
df["weather_main"].value_counts()
# df["weather_description"].value_counts()

"""# Datetime dan Feature Engineering

Karena terdapat atribut datetime, kita perlu memproses lebih lanjut data tersebut untuk mendapatkan insight terbaru.
Kita dapat mengambil data jam, Hari/Tanggal_hari, bulan, tahun.
"""

df["date_time"] = pd.to_datetime(df["date_time"],
                                 format='%Y-%m-%d %H:%M:%S',
                                 errors='coerce')

"""Data sudah menjadi tipe datetime"""

df["date_time"].tail(5)

"""Jika diperiksa secara seksama, nilai Menit dan Detik selalu 0 jadi bisa kita abaikan"""

## Hal ini akan berakibat bertambahnya attribut
df["date_time_year"] = df["date_time"].dt.year
df["date_time_month"] = df["date_time"].dt.month
df["date_time_day"] = df["date_time"].dt.day
df["date_time_hour"] = df["date_time"].dt.hour

"""Jika diperiksa lebih lanjut, setiap tahun, bulan, hari, dan jam lengkap"""

df.info()

df.groupby("date_time_year").size()
# df.groupby("date_time_month").size()
# df.groupby("date_time_day").size()
# df.groupby("date_time_hour").size()

"""# Univariate Analysis

## Melihat Lebih Jauh Menggunakan Histogram pada Fitur Numerik

Melihat lebih jelas memanfaatkan histogram, histogram menunjukan banyak instansi terhadap rentang nilai pada atribut tersebut.
Sejauh ini insight yang didapat adalah:
1. Skala yang digunakan berbeda beda
2. Pada rain_1h dan snow_1h hanya memilki satu rentang saja
"""

df.hist(bins=100, figsize=(20, 15))
plt.show()

"""## Menganalisis Fitur Kategorikal Menggunakan Bar Chart

1. Melihat ketersebaran data pada setiap kategori
"""

df["weather_main"].value_counts().plot(kind="bar", title="Weather Main")

# df["weather_description"].value_counts()
df["weather_description"].value_counts().plot(kind="bar", title="Weather Description")

df["holiday"].value_counts().plot(kind="bar", title="Holiday")

"""# Multivariate Analysis

## Mencari Korelasi Pada Fitur Numerik

Machine learning akan bekerja lebih baik pada fitur-fitur yang memiliki korelasi linear yang kuat
Dapat dilihat bahwa nilai korelasi yang ada pada dataset terhadap atribut label.

Nilai korelasi linear ini jatuh pada rentang -1 dan 1. Dimana semakin mendekati 1 berarti memiliki korelasi linear positif yang kuat dan mendekati -1 memiliki korelasi linear negatif yang kuat
"""

corr_matrix = df.corr()
corr_matrix["traffic_volume"].sort_values(ascending=False)

numerical_attributes = ["temp", "date_time_hour", "clouds_all", 
                        "traffic_volume"]

scatter_matrix(df[numerical_attributes], figsize=(12, 8))

"""# Melihat Lebih Jauh

Jika dilihat hubungan antara fitur date_time_hour dengan fitur target traffic_volume memiliki hubungan non-linear
"""

df.plot(kind="scatter", x="date_time_hour", y="traffic_volume", alpha=0.1)

"""## Analisis Fitur Kategorikal

Memperhatikan representasi yang ada, dapat disimpulkan bahwa:
1. Volume Traffic cenderung menurun saat tidak ada hari libur
2. Volume Traffic juga cenderung menurun ketika terjadi badai
"""

categorical_features = ["holiday", "weather_main", "weather_description"]

col = categorical_features[0]

sns.catplot(x=col, y="traffic_volume", kind="bar",
            dodge=False, height=4, aspect=3,
            data=df, palette="Set3", legend=True)
plt.title("Rata-rata Volume Traffic terhadap - {}".format(col))

col = categorical_features[1]

sns.catplot(x="traffic_volume", y=col, kind="bar",
            dodge=False, height=6, aspect=3,
            data=df, palette="Set3", legend=True)
plt.title("Rata-rata Volume Traffic terhadap - {}".format(col))

col = categorical_features[2]

sns.catplot(x="traffic_volume", y=col, kind="bar",
            dodge=False, height=10, aspect=3,
            data=df, palette="Set3", legend=True)
plt.title("Rata-rata Volume Traffic terhadap - {}".format(col))

"""## Kesimpulan Visualisasi Data

Setelah melihat dan memahami data, untuk sementara dapat disimpulkan bahwa terdapat beberapa fitur yang perlu disimpan dan sisanya di-drop. Untuk fitur numerik yang memiliki nilai koefisien korelasi sangat mendekati 0 akan didrop. Setelah ini akan dilanjutkan pada Data Preparation

# Data Preparation

* Split the data
* Merubah tipe data kategori menjadi numerik
* Menstardarisasi tipe data numerik

## Split the Data

Membagi data dari training dan testing dengan rasio 90:10
"""

df["weather_main"].value_counts()

columns = ["weather_main"]
df_reset_index = df.reset_index()

split = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=0)
for train_index, test_index in split.split(df_reset_index, df_reset_index[columns]):
    strat_train_set = df_reset_index.loc[train_index]
    strat_test_set = df_reset_index.loc[test_index]

feature_columns = ["temp", "date_time_hour", "weather_main"]
label_columns = ["traffic_volume"]

X_train = strat_train_set[feature_columns]
y_train = strat_train_set[label_columns]

X_test = strat_test_set[feature_columns]
y_test = strat_test_set[label_columns]

print(X_train.shape)
print(X_test.shape)

"""## Handling Categorical Data

Pada bagian ini akan dilakukan perubahan data kategorikal menjadi data numerik menggunakan metode OneHotEncoding.
"""

cat_encoder = OneHotEncoder()

weather_desc_cat = X_train[["weather_main"]]
wd_cat_1hot = cat_encoder.fit_transform(weather_desc_cat)

wd_cat_1hot.shape

"""## Normalisasi/Standarisasi Data

Karena data memiliki skala yang berbeda, alangkah lebih baik untuk dinormalisasikan/distandarisasikan
"""

num_columns = ["temp", "date_time_hour"]

numerical_pipeline = Pipeline([
    ("minmax_scaler", MinMaxScaler())
])

df_num = numerical_pipeline.fit_transform(X_train[num_columns])
df_num.shape

"""## Transformation Pipeline

Pada bagian ini adalah pengaplikasian dari feature scaling dan perubahan kategorikal data memanfaatkan pipeline yang disediakan oleh library Sckit Learn.
"""

num_columns = ["temp", "date_time_hour"]
cat_columns = columns

transform_pipeline = ColumnTransformer([
    ("numeric", numerical_pipeline, num_columns),
    ("categorical", OneHotEncoder(), cat_columns)
])

X_train_prepared = transform_pipeline.fit_transform(X_train)
y_train = np.reshape(y_train.to_numpy(), y_train.shape[0])
X_test_prepared = transform_pipeline.fit_transform(X_test)
y_test = np.reshape(y_test.to_numpy(), y_test.shape[0])

print("The train data sparse matrix is in shape of {}".format(X_train_prepared.shape))
print("The test data sparse matrix is in shape of {}".format(X_test_prepared.shape))

"""# Modelling

Model yang digunakan antar lain sebagai berikut:
* Linear Regression
* Decision Tree Regressor
* Random Forest Regressor
"""

df_models = pd.DataFrame(index=["LinearRegression", "DTRegressor", "RFRegressor"],
                         columns=["train_rmse", "test_rmse"])

def evaluation(model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)
    scores = cross_val_score(model, X_train, y_train,
                             scoring="neg_root_mean_squared_error",
                             cv=8)
    train_rmse = -scores.mean()
    
    tv_prediction = model.predict(X_test)
    test_rmse = np.sqrt(mean_squared_error(y_test, tv_prediction))
    return train_rmse, test_rmse

## fitting the data Linear Regression
lin_reg = LinearRegression()

train_rmse, test_rmse = evaluation(lin_reg, X_train_prepared, y_train, X_test_prepared, y_test)

df_models.loc["LinearRegression", "test_rmse"] = test_rmse
df_models.loc["LinearRegression", "train_rmse"] = train_rmse

df_models

# fitting the data using DTRegressor
tree_reg = DecisionTreeRegressor()

train_rmse, test_rmse = evaluation(tree_reg, X_train_prepared, y_train, X_test_prepared, y_test)

print(train_rmse)
print(test_rmse)
df_models.loc["DTRegressor", "test_rmse"] = test_rmse
df_models.loc["DTRegressor", "train_rmse"] = train_rmse

df_models

## fitting the data using RFRegressor
forest_reg = RandomForestRegressor()

train_rmse, test_rmse = evaluation(forest_reg, X_train_prepared, y_train, X_test_prepared, y_test)

df_models.loc["RFRegressor", "test_rmse"] = test_rmse
df_models.loc["RFRegressor", "train_rmse"] = train_rmse

df_models

"""# Visualisasi Hasil Training Model

Berikut adalah visualisasi perbandingan metrik evaluasi setiap model.
"""

fig, ax = plt.subplots()
df_models.sort_values(by='test_rmse', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""# Mencari Parameter Terbaik Dengan GridSearch

Parameter terbaik yang ditemukan dengan menggunakan cross validation adalah sebagai berikut
"""

param_grid = [
    {'n_estimators': [3, 10, 30, 50, 70, 100], 'max_features': [2, 4, 6]},
    {'bootstrap': [False], 'n_estimators': [3, 10, 30, 50], 'max_features': [2, 3, 4]},
]

forest_reg_gs = RandomForestRegressor()

grid_search = GridSearchCV(forest_reg_gs, param_grid, cv=5,
                           scoring='neg_root_mean_squared_error',
                           return_train_score=True,
                           verbose=2,)

grid_search.fit(X_train_prepared, y_train)

grid_search.best_params_

cv_res = grid_search.cv_results_
for rmse, params in zip(cv_res["mean_test_score"], cv_res["params"]):
    print((-rmse), params)

# fitting the data using RFRegressor
forest_reg_ht = RandomForestRegressor(max_features=6, n_estimators=100)

train_rmse, test_rmse = evaluation(forest_reg_ht, X_train_prepared, y_train, X_test_prepared, y_test)
print(train_rmse, test_rmse)

df_models.loc["RFRegressorHT", "test_rmse"] = test_rmse
df_models.loc["RFRegressorHT", "train_rmse"] = train_rmse

df_models

"""# Visualisasi Perbandingan Model Dengan Hyperparameter Tuning

Berikut menambahkan model Random Forest Regressor memanfaatkan hyperparameter tuning menggunakan metode grid search
"""

fig, ax = plt.subplots()
df_models.sort_values(by='test_rmse', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)